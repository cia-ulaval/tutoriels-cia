
@misc{li_mmd_2017,
	title = {{MMD} {GAN}: {Towards} {Deeper} {Understanding} of {Moment} {Matching} {Network}},
	shorttitle = {{MMD} {GAN}},
	url = {http://arxiv.org/abs/1705.08584},
	abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efﬁciency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efﬁciency by introducing adversarial kernel learning techniques, as the replacement of a ﬁxed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and is competitive with other representative GAN works.},
	language = {en},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and Póczos, Barnabás},
	month = nov,
	year = {2017},
	note = {arXiv:1705.08584 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf:/Users/mathieubazinet/Zotero/storage/38WZV5LZ/Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf:application/pdf},
}

@misc{mbacke_pac-bayesian_2023,
	title = {{PAC}-{Bayesian} {Generalization} {Bounds} for {Adversarial} {Generative} {Models}},
	url = {http://arxiv.org/abs/2302.08942},
	abstract = {We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Mbacke, Sokhna Diarra and Clerc, Florence and Germain, Pascal},
	month = jul,
	year = {2023},
	note = {arXiv:2302.08942 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/mathieubazinet/Zotero/storage/2EQNLIH6/Mbacke et al. - 2023 - PAC-Bayesian Generalization Bounds for Adversarial.pdf:application/pdf;arXiv.org Snapshot:/Users/mathieubazinet/Zotero/storage/CWF4PIWI/2302.html:text/html},
}

@misc{binkowski_demystifying_2021,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Bińkowski, Mikołaj and Sutherland, Danica J. and Arbel, Michael and Gretton, Arthur},
	month = jan,
	year = {2021},
	note = {arXiv:1801.01401 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mathieubazinet/Zotero/storage/YB4XUIH2/Bińkowski et al. - 2021 - Demystifying MMD GANs.pdf:application/pdf;arXiv.org Snapshot:/Users/mathieubazinet/Zotero/storage/AN67FYJN/1801.html:text/html},
}

@article{asokan_spider_nodate,
	title = {Spider {GAN}: {Leveraging} {Friendly} {Neighbors} to {Accelerate} {GAN} {Training} {Supporting} {Document}},
	shorttitle = {Spider {GAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/supplemental/Asokan_Spider_GAN_Leveraging_CVPR_2023_supplemental.pdf},
	urldate = {2024-04-09},
	author = {Asokan, Siddarth and Seelamantula, Chandra Sekhar},
	file = {Available Version (via Google Scholar):/Users/mathieubazinet/Zotero/storage/ZLI9NGXV/Asokan et Seelamantula - Spider GAN Leveraging Friendly Neighbors to Accel.pdf:application/pdf},
}

@inproceedings{asokan_spider_2023,
	address = {Vancouver, BC, Canada},
	title = {Spider {GAN}: {Leveraging} {Friendly} {Neighbors} to {Accelerate} {GAN} {Training}},
	isbn = {9798350301298},
	shorttitle = {Spider {GAN}},
	url = {https://ieeexplore.ieee.org/document/10204936/},
	doi = {10.1109/CVPR52729.2023.00378},
	abstract = {Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efﬁcient by identifying closely related datasets, or a “friendly neighborhood” of the target distribution, inspiring the moniker, Spider GAN. To deﬁne friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between TinyImageNet and CelebA faces. Further, we demonstrate cascading Spider GAN, where the output distribution from a pre-trained GAN generator is used as the input to the subsequent network. Effectively, transporting one distribution to another in a cascaded fashion until the target is learnt – a new ﬂavor of transfer learning. We demonstrate the efﬁcacy of the Spider approach on DCGAN, conditional GAN, PGGAN, StyleGAN2 and StyleGAN3. The proposed approach achieves state-of-the-art Fréchet inception distance (FID) values, with one-ﬁfth of the training iterations, in comparison to their baseline counterparts on high-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Asokan, Siddarth and Seelamantula, Chandra Sekhar},
	month = jun,
	year = {2023},
	pages = {3883--3893},
	file = {Asokan et Seelamantula - 2023 - Spider GAN Leveraging Friendly Neighbors to Accel.pdf:/Users/mathieubazinet/Zotero/storage/KXBVGKEP/Asokan et Seelamantula - 2023 - Spider GAN Leveraging Friendly Neighbors to Accel.pdf:application/pdf},
}

@inproceedings{cui_kd-dlgan_2023,
	address = {Vancouver, BC, Canada},
	title = {{KD}-{DLGAN}: {Data} {Limited} {Image} {Generation} via {Knowledge} {Distillation}},
	isbn = {9798350301298},
	shorttitle = {{KD}-{DLGAN}},
	url = {https://ieeexplore.ieee.org/document/10203862/},
	doi = {10.1109/CVPR52729.2023.00377},
	abstract = {Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledgedistillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cui, Kaiwen and Yu, Yingchen and Zhan, Fangneng and Liao, Shengcai and Lu, Shijian and Xing, Eric},
	month = jun,
	year = {2023},
	pages = {3872--3882},
	file = {Cui et al. - 2023 - KD-DLGAN Data Limited Image Generation via Knowle.pdf:/Users/mathieubazinet/Zotero/storage/3PCING78/Cui et al. - 2023 - KD-DLGAN Data Limited Image Generation via Knowle.pdf:application/pdf},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jan,
	year = {2017},
	file = {Full Text PDF:/Users/mathieubazinet/Zotero/storage/SNZKJMLC/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf},
}

@inproceedings{mroueh_convergence_2021,
	title = {On the {Convergence} of {Gradient} {Descent} in {GANs}: {MMD} {GAN} {As} a {Gradient} {Flow}},
	shorttitle = {On the {Convergence} of {Gradient} {Descent} in {GANs}},
	url = {https://proceedings.mlr.press/v130/mroueh21a.html},
	abstract = {We consider the maximum mean discrepancy MMD GAN problem and propose a parametric kernelized gradient flow that mimics the min-max game in gradient regularized MMD GAN. We show that this flow provides a descent direction minimizing the MMD on a statistical manifold of probability distributions. We then derive an explicit condition which ensures that gradient descent on the parameter space of the generator in gradient regularized MMD GAN is globally convergent to the target distribution. Under this condition , we give non asymptotic convergence results for MMD GAN. Another contribution of this paper is the introduction of a dynamic formulation of a regularization of MMD and demonstrating that the parametric kernelized descent for MMD is the gradient flow of this functional with respect to the new Riemannian structure. Our obtained theoretical result allows ones to treat gradient flows for quite general functionals and thus has potential applications to other types of variational inferences on a statistical manifold beyond GANs. Finally, numerical experiments suggest that our parametric kernelized gradient flow stabilizes GAN training and guarantees convergence.},
	language = {en},
	urldate = {2023-12-21},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Mroueh, Youssef and Nguyen, Truyen},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1720--1728},
	file = {Full Text PDF:/Users/mathieubazinet/Zotero/storage/6JIECURB/Mroueh et Nguyen - 2021 - On the Convergence of Gradient Descent in GANs MM.pdf:application/pdf},
}
