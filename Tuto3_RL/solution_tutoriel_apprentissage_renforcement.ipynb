{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9be702-842c-4385-b00f-e87984963db6",
   "metadata": {},
   "source": [
    "# Apprentissage par renforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2109f046-5a00-48c9-abb2-28bd5546267c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:15:10.290467Z",
     "iopub.status.busy": "2025-10-06T15:15:10.289965Z",
     "iopub.status.idle": "2025-10-06T15:15:10.491659Z",
     "shell.execute_reply": "2025-10-06T15:15:10.490977Z",
     "shell.execute_reply.started": "2025-10-06T15:15:10.290417Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import lightning as L\n",
    "from torch.distributions import MultivariateNormal\n",
    "from tqdm import tqdm, trange\n",
    "from utils import get_device, verify_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfaf220-f9e6-47e8-be00-5b88fd7f5d29",
   "metadata": {},
   "source": [
    "# Essayez les jeux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158f52-c34b-4c84-805f-32c48158051d",
   "metadata": {},
   "source": [
    "Vous pouvez essayer un jeu, vous avez le choix entre \"Car\", \"Mountain\" et \"Lunar\". Vous pouvez jouer avec les touches \"a\", \"s\", \"d\" et \"w\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebad210-4d01-4432-b316-44518ba0e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python play.py --game=\"Mountain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26807f52-1f19-409f-828c-4221a8aa6a73",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bd134-ccb4-4645-83a9-51f926ef44f6",
   "metadata": {},
   "source": [
    "Cette section du laboratoire est fortement inspirée de ces deux tutoriels suivants : \n",
    "1. https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm\n",
    "\n",
    "2. https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2aa06-fd5b-46a3-b2a0-d7248a3acd13",
   "metadata": {},
   "source": [
    "<img src=\"./images/deep-q-learning.jpg\" width=\"70%\" class=\"center\"/>\n",
    "\n",
    "[source](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1808ca-66a6-48c5-a47a-fa23ebf64aa1",
   "metadata": {},
   "source": [
    "Commençons par implémenter le *replay buffer*. Le buffer a une taille maximale, donc lorsque le buffer est plein, on efface les données les plus anciennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd024cb-ee77-4709-af43-3b90f03a16e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:20:16.908172Z",
     "iopub.status.busy": "2025-10-06T14:20:16.907595Z",
     "iopub.status.idle": "2025-10-06T14:20:16.916586Z",
     "shell.execute_reply": "2025-10-06T14:20:16.915572Z",
     "shell.execute_reply.started": "2025-10-06T14:20:16.908116Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_capacity):\n",
    "        self.memory = [] # Choisissez la structure de données que vous préférez\n",
    "        self.max_capacity = max_capacity\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        transition = Transition(state, action, next_state, reward)\n",
    "        # Ajoutez les données à la mémoire, en s'assurant que le buffer n'est pas plein.\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.max_capacity:\n",
    "            val = self.memory.pop(0)\n",
    "            assert len(self.memory) == self.max_capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135f4ec-abdc-463b-b50a-7b76d67bebed",
   "metadata": {},
   "source": [
    "On va maintenant implémenter le réseau de neurones. Ils suggèrent un réseau de neurones avec deux couches cachées de 128 neurones, donc c'est ce qu'on utilise ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "642b559f-408d-4b24-9a6e-8efbf4be68de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:20:18.520526Z",
     "iopub.status.busy": "2025-10-06T14:20:18.519980Z",
     "iopub.status.idle": "2025-10-06T14:20:18.527290Z",
     "shell.execute_reply": "2025-10-06T14:20:18.526441Z",
     "shell.execute_reply.started": "2025-10-06T14:20:18.520473Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_obs, n_actions, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.n_obs = n_obs\n",
    "        self.n_actions = n_actions\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_obs, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, self.n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c1fa0-c070-47f1-9be9-a4f1476bc133",
   "metadata": {},
   "source": [
    "On implémente l'étape d'entraînement de l'algorithme présenté plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fdac69e-269c-4a03-a092-ccf07e91035d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:20:20.139423Z",
     "iopub.status.busy": "2025-10-06T14:20:20.138910Z",
     "iopub.status.idle": "2025-10-06T14:20:20.148841Z",
     "shell.execute_reply": "2025-10-06T14:20:20.147963Z",
     "shell.execute_reply.started": "2025-10-06T14:20:20.139386Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_model(model, frozen_model, batch_size, memory, gamma, optimizer):\n",
    "    model.train()\n",
    "    # Allez chercher une batch dans la mémoire\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    all_states = torch.tensor(np.stack(batch.state), device=get_device())\n",
    "    all_next_states = batch.next_state\n",
    "\n",
    "    # Créez un tenseur qui vérifie que next_state est un état valide et non un état terminal. Il doit contenir\n",
    "    # True si next_state est valide.\n",
    "    mask = torch.tensor([next_state is not None for next_state in all_next_states]\n",
    "                        , dtype=torch.bool, device=get_device())\n",
    "\n",
    "    # Créez un tenseur qui contient tous les next_state valides .\n",
    "    non_final_next_states = torch.tensor(np.stack([next_state for next_state in all_next_states if next_state is not None]\n",
    "                                                 ), device=get_device())\n",
    "\n",
    "    rewards = torch.tensor(batch.reward, device=get_device())\n",
    "    actions = torch.tensor(batch.action, device=get_device()).unsqueeze(1)\n",
    "    targets = rewards\n",
    "    with torch.no_grad():\n",
    "        targets[mask] += gamma * frozen_model(non_final_next_states).max(1).values\n",
    "    ys = model(all_states).gather(1, actions).reshape(-1,)\n",
    "\n",
    "    # Calculez la moyenne de la distance au carré entre targets et ys.\n",
    "    loss = (targets - ys).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # On clip le gradient pour assurer la stabilité\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2765229-d272-48c6-bb5a-fd9f5e2f3424",
   "metadata": {},
   "source": [
    "On implémente maintenant la boucle d'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a3f066-6e20-4e41-a3f1-2a024f96edbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:20:22.437802Z",
     "iopub.status.busy": "2025-10-06T14:20:22.437346Z",
     "iopub.status.idle": "2025-10-06T14:20:22.447616Z",
     "shell.execute_reply": "2025-10-06T14:20:22.446752Z",
     "shell.execute_reply.started": "2025-10-06T14:20:22.437771Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(env, config):\n",
    "    obs, info = env.reset()\n",
    "    size_observation_space = len(obs)\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    epsilon = config['epsilon_start']\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    memory = ReplayMemory(max_capacity=config['max_capacity'])\n",
    "    # Initialize action-value function Q with random weights theta\n",
    "    Q_model = DQN(size_observation_space, n_actions, config['hidden_layer_size'])\n",
    "\n",
    "    # Initialize target action-value function \\hat{Q} with weights theta^-1 = theta\n",
    "    Q_hat_model = deepcopy(Q_model)\n",
    "\n",
    "    Q_model.eval()\n",
    "    Q_hat_model.eval()\n",
    "    Q_model.to(get_device())\n",
    "    Q_hat_model.to(get_device())\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_model.parameters(), lr=config['lr'])\n",
    "\n",
    "    global_step_count = 0\n",
    "    for episode_num in range(config['num_eval_episodes']):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        # On crée notre distribution pour le compromis exploration-exploitation\n",
    "        epsilon_greedy = torch.distributions.bernoulli.Bernoulli(torch.tensor([epsilon]))\n",
    "        for n_step in range(env.spec.max_episode_steps):\n",
    "            # Add epsilon decay\n",
    "            if bool(epsilon_greedy.sample().item()):\n",
    "                action = env.action_space.sample()  # Random policy for demonstration\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(Q_model(torch.tensor(obs, device=get_device()))).item()\n",
    "\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_terminated = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            global_step_count += 1\n",
    "\n",
    "            # On ajoute les données dans la mémoire. Si l'état est terminal, on ajoute None au lieu de l'état\n",
    "            if episode_terminated:\n",
    "                memory.push(obs, action, None, reward)\n",
    "            else:\n",
    "                memory.push(obs, action, new_obs, reward)\n",
    "            obs = new_obs\n",
    "\n",
    "            # Dès qu'on a assez de données, on se met à entraîner le modèle\n",
    "            if len(memory) >= config['batch_size']:\n",
    "                update_model(Q_model, Q_hat_model, config['batch_size'], memory, config['gamma'], optimizer)\n",
    "\n",
    "            # Après plusieurs étapes, on met à jour notre modèle gelé\n",
    "            if global_step_count % config['n_step_update'] == 0:\n",
    "                Q_hat_model = deepcopy(Q_model)\n",
    "\n",
    "            if episode_terminated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode_num + 1}: {step_count} steps, reward = {episode_reward}\")\n",
    "        epsilon *= pow(config['epsilon_end']/config['epsilon_start'], 1/num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bcb2a-7699-4126-80e7-dd5685744662",
   "metadata": {},
   "source": [
    "Voici le code qui permet de rouler l'entièreté de l'algorithme. Vous pouvez jouer avec les hyperparamètres, mais ceux que j'ai utilisés fonctionnent bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2fac59-4237-4ba1-8760-dd763ba709c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:20:27.638472Z",
     "iopub.status.busy": "2025-10-06T14:20:27.638037Z",
     "iopub.status.idle": "2025-10-06T14:20:31.658818Z",
     "shell.execute_reply": "2025-10-06T14:20:31.658425Z",
     "shell.execute_reply.started": "2025-10-06T14:20:27.638442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 5 episodes...\n",
      "Episode 1: 200 steps, reward = -200.0\n",
      "Episode 2: 200 steps, reward = -200.0\n",
      "Episode 3: 200 steps, reward = -200.0\n",
      "Episode 4: 200 steps, reward = -200.0\n",
      "Episode 5: 200 steps, reward = -200.0\n",
      "\n",
      "Average reward: -200.00 ± 0.00\n",
      "Average episode length: 200.0 steps\n",
      "Success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(42, workers=True)\n",
    "\n",
    "num_eval_episodes = 450\n",
    "env_name = \"MountainCar-v0\"\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 10 == 0    # Record every episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} episodes...\")\n",
    "\n",
    "config = {\n",
    "    \"num_eval_episodes\":num_eval_episodes,\n",
    "    \"n_step_update\": 10,\n",
    "    \"max_capacity\" : 10000,\n",
    "    \"batch_size\" : 128,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"lr\" : 3e-4,\n",
    "    \"epsilon_start\" : 0.9,\n",
    "    \"epsilon_end\" : 0.01,\n",
    "    \"hidden_layer_size\":128,\n",
    "}\n",
    "\n",
    "train_model(env, config)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.mean(env.return_queue)\n",
    "avg_length = np.mean(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > -200) / len(env.return_queue):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1f41b-0b4f-4f1e-992e-07b95ba271d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:30:19.511896Z",
     "iopub.status.busy": "2025-09-21T15:30:19.511255Z",
     "iopub.status.idle": "2025-09-21T15:31:06.262141Z",
     "shell.execute_reply": "2025-09-21T15:31:06.261854Z",
     "shell.execute_reply.started": "2025-09-21T15:30:19.511849Z"
    }
   },
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df2527-2e4c-4e1f-83ae-51bf76248ac4",
   "metadata": {},
   "source": [
    "Cette section du laboratoire est fortement inspirée de cette implémentation : \n",
    "1. https://github.com/ericyangyu/PPO-for-Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563fbcd-2789-481a-a732-7ff187e4f6ce",
   "metadata": {},
   "source": [
    "<img src=\"./images/ppo_algo.png\" width=\"70%\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474db46-1ecc-4111-9392-4301afc56b6d",
   "metadata": {},
   "source": [
    "Les deux réseaux de neurones ont la même architecture, donc j'ai créé seulement une classe pour faire les deux réseaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ff3a3f1-8a7b-4aae-9121-4cb0227de90d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.412679Z",
     "iopub.status.busy": "2025-10-06T15:16:45.412043Z",
     "iopub.status.idle": "2025-10-06T15:16:45.423587Z",
     "shell.execute_reply": "2025-10-06T15:16:45.422763Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.412626Z"
    }
   },
   "outputs": [],
   "source": [
    "class PPONetwork(torch.nn.Module):\n",
    "    def __init__(self, n_obs, out_dim, hidden_size=64, std_val=0.5):\n",
    "        super().__init__()\n",
    "        self.n_obs = n_obs\n",
    "        self.out_dim = out_dim\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_obs, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, self.out_dim)\n",
    "        )\n",
    "        self.cov_mat = torch.diag(torch.full(size=(self.out_dim,), fill_value=std_val))\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "    def get_action(self, input):\n",
    "        moyenne = self(input)\n",
    "        dist = MultivariateNormal(moyenne, self.cov_mat.to(get_device()))\n",
    "\n",
    "        # On pige une action dans la distribution\n",
    "        action = dist.sample()\n",
    "\n",
    "        # On calcule la log_probabilité de l'action en fonction de la distribution\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.cpu().numpy(), log_prob.detach().cpu().item()\n",
    "\n",
    "    def evaluate(self, input, actions):\n",
    "        dist = MultivariateNormal(self(input), self.cov_mat.to(get_device()))\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853deee-9755-4ed9-8506-e35fca2637dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:06:23.657643Z",
     "iopub.status.busy": "2025-10-06T14:06:23.655856Z",
     "iopub.status.idle": "2025-10-06T14:06:23.667876Z",
     "shell.execute_reply": "2025-10-06T14:06:23.666699Z",
     "shell.execute_reply.started": "2025-10-06T14:06:23.657559Z"
    }
   },
   "source": [
    "On va maintenant estimer les \"avantages\". Pour ce faire, on va calculer la somme actualisée avec le paramètre gamma pour chaque épisodes. \n",
    "<img src=\"./images/advantages.png\" width=\"50%\" class=\"center\"/>\n",
    "\n",
    "Plus exactement, pour chaque épisode, on va chercher à calculer :\n",
    "$\\sum_{t=0}^T \\gamma^t r_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95769853-7743-41f0-8d38-6d854c8289bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.426197Z",
     "iopub.status.busy": "2025-10-06T15:16:45.425966Z",
     "iopub.status.idle": "2025-10-06T15:16:45.439873Z",
     "shell.execute_reply": "2025-10-06T15:16:45.439258Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.426179Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_estimated_advantages(batch_rewards, gamma):\n",
    "    batch_rtgs = []\n",
    "    # On calcule les rewards diminués pour chaque épisode\n",
    "    for ep_rewards in reversed(batch_rewards):\n",
    "\n",
    "        discounted_reward = 0  # Somme des rewards diminués\n",
    "\n",
    "        # On itère sur tous les rewards de l'épisode\n",
    "        for rew in reversed(ep_rewards):\n",
    "            # On additionne le reward actual avec les rewards obtenus précédemment multipliés par gamma.\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "    return batch_rtgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05dffea-1a0e-4738-82f6-10e4489f7f85",
   "metadata": {},
   "source": [
    "On fait jouer plein d'agents différents pour obtenir les données sur lesquelles on va s'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae43309e-2f2c-405c-8363-67f0c82617c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.452634Z",
     "iopub.status.busy": "2025-10-06T15:16:45.452288Z",
     "iopub.status.idle": "2025-10-06T15:16:45.467694Z",
     "shell.execute_reply": "2025-10-06T15:16:45.467060Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.452612Z"
    }
   },
   "outputs": [],
   "source": [
    "def rollout(env, n_actors, iter_number, actor, critic):\n",
    "    batch_obs = []\n",
    "    batch_actions = []\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "\n",
    "    for n_actor in trange(n_actors, desc=\"Actors\"):\n",
    "        obs, info = env.reset(seed=(iter_number+1)*(n_actor+1))\n",
    "        ep_rewards = []\n",
    "        for n_step in range(env.spec.max_episode_steps):\n",
    "            action, log_prob = actor.get_action(torch.tensor(obs, device=get_device()))\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            ep_rewards.append(reward)\n",
    "            batch_obs.append(obs)\n",
    "            batch_actions.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "            episode_terminated = terminated or truncated\n",
    "\n",
    "            if episode_terminated:\n",
    "                break\n",
    "        batch_rewards.append(ep_rewards)\n",
    "\n",
    "    batch_obs = torch.tensor(np.array(batch_obs), dtype=torch.float)\n",
    "    batch_actions = torch.tensor(np.array(batch_actions), dtype=torch.float)\n",
    "    batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float)\n",
    "    \n",
    "    return batch_obs, batch_actions, batch_log_probs, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ca608-d70d-4cd2-8cda-65d61e3137fd",
   "metadata": {},
   "source": [
    "On met maintenant à jour les réseaux de neurones. On calcule la perte avec l'équation suivante. Le ratio $r_t$ est calculé pour vous.\n",
    "\n",
    "<img src=\"./images/l_clip.png\" width=\"50%\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c741285b-f50a-4f01-b6f3-4dc17bd9cd06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.468603Z",
     "iopub.status.busy": "2025-10-06T15:16:45.468474Z",
     "iopub.status.idle": "2025-10-06T15:16:45.474932Z",
     "shell.execute_reply": "2025-10-06T15:16:45.474404Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.468591Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_models(dataloader, actor, critic, optimizer_actor, optimizer_critic, epsilon, n_epochs):\n",
    "        actor.train()\n",
    "        critic.train()\n",
    "        device = get_device()\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for _ in trange(n_epochs, desc=\"Epochs\"):\n",
    "            for batch in dataloader:\n",
    "                b_obs = batch[0].to(device)\n",
    "                b_acts = batch[1].to(device)\n",
    "                b_log_probs = batch[2].to(device)\n",
    "                b_rtgs = batch[3].to(device)\n",
    "                b_ak = batch[4].to(device)\n",
    "                V = critic(b_obs).squeeze()\n",
    "                curr_log_probs = actor.evaluate(b_obs, b_acts)\n",
    "                ratios = torch.exp(curr_log_probs - b_log_probs)\n",
    "\n",
    "                # On calcule la première partie de la perte \n",
    "                surr1 = ratios * b_ak\n",
    "                # On calcule la deuxième partie de la perte, avec la fonction torch.clamp\n",
    "                surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * b_ak\n",
    "\n",
    "                # On calcule la perte pour l'acteur\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                optimizer_actor.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                optimizer_actor.step()\n",
    "\n",
    "                # On calcule la perte pour le critique\n",
    "                V, b_rtgs = verify_shape(V, b_rtgs)\n",
    "                critic_loss = loss_fn(V, b_rtgs)\n",
    "\n",
    "                optimizer_critic.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                optimizer_critic.step()\n",
    "        actor.eval()\n",
    "        critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2085-3a0c-4949-a3e1-69d62deb2856",
   "metadata": {},
   "source": [
    "Voici la boucle d'entraînement. On fait notre \"rollout\", où on crée les données sur lesquelles on va s'entraîner. Ensuite, on estimer l'avantage et on entraîne le modèle, avec les méthodes qu'on a implémentés plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "201b5bfb-dfa7-4435-b741-3817bef54f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.476421Z",
     "iopub.status.busy": "2025-10-06T15:16:45.476282Z",
     "iopub.status.idle": "2025-10-06T15:16:45.482619Z",
     "shell.execute_reply": "2025-10-06T15:16:45.482108Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.476408Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(env, config):\n",
    "    obs, info = env.reset()\n",
    "    size_observation_space = len(obs)\n",
    "    n_actions = env.action_space.shape[0]\n",
    "\n",
    "    actor = PPONetwork(size_observation_space, n_actions, config['hidden_layer_size'])\n",
    "    critic = PPONetwork(size_observation_space, 1, config['hidden_layer_size'])\n",
    "\n",
    "    optimizer_actor = torch.optim.Adam(actor.parameters(), lr=config['lr'], eps=1e-5)\n",
    "    optimizer_critic = torch.optim.Adam(critic.parameters(), lr=config['lr'], eps=1e-5)\n",
    "\n",
    "    actor.to(get_device())\n",
    "    actor.eval()\n",
    "    critic.to(get_device())\n",
    "    critic.eval()\n",
    "\n",
    "    for i in range(config['num_eval_episodes']):\n",
    "        batch_obs, batch_actions, batch_log_probs, batch_rewards = rollout(env, config['n_actors'], i, actor, critic)\n",
    "\n",
    "        final_rewards = np.array([sum(ep_rewards) for ep_rewards in batch_rewards])\n",
    "        print(f\"Average reward : {final_rewards.mean():.2f}   Max final reward : {final_rewards.max():.2f}\")\n",
    "        print(f\"Pourcentage de réussite après {i} itérations: {100*np.count_nonzero(final_rewards>=200)/final_rewards.shape[0]:.2f}%\")\n",
    "\n",
    "        batch_rtgs = calculate_estimated_advantages(batch_rewards, config['gamma'])\n",
    "        \n",
    "        V = critic(batch_obs.to(get_device())).squeeze()\n",
    "        A_k = batch_rtgs.to(get_device()) - V.detach()\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # Normalization trick\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(batch_obs, batch_actions, batch_log_probs, batch_rtgs, A_k)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,batch_size=config['batch_size'], shuffle=True)\n",
    "        update_models(dataloader, actor, critic, optimizer_actor, optimizer_critic, config['epsilon'], config['n_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb3b33-1e69-4cc2-b038-41e30b0d23ad",
   "metadata": {},
   "source": [
    "Voici le code qui permet de rouler l'entièreté de l'algorithme. Vous pouvez jouer avec les hyperparamètres, mais ceux que j'ai utilisés fonctionnent bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10f65fd1-dd8d-464c-97d9-a892cf682097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:16:45.483642Z",
     "iopub.status.busy": "2025-10-06T15:16:45.483506Z",
     "iopub.status.idle": "2025-10-06T15:17:04.069449Z",
     "shell.execute_reply": "2025-10-06T15:17:04.068832Z",
     "shell.execute_reply.started": "2025-10-06T15:16:45.483630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 10 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actors: 100%|███████████████████████████████████| 20/20 [00:08<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward : -324.41   Max final reward : -20.32\n",
      "Pourcentage de réussite après 0 itérations: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████████████████████████████████████| 5/5 [00:02<00:00,  2.44it/s]\n",
      "Actors: 100%|███████████████████████████████████| 20/20 [00:06<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward : -263.30   Max final reward : -45.23\n",
      "Pourcentage de réussite après 1 itérations: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████████████████████▏              | 3/5 [00:00<00:00,  3.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting evaluation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_eval_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m config = {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_eval_episodes\u001b[39m\u001b[33m\"\u001b[39m:num_eval_episodes,\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_actors\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhidden_layer_size\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m64\u001b[39m,\n\u001b[32m     31\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m env.close()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Calculate some useful metrics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(env, config)\u001b[39m\n\u001b[32m     30\u001b[39m dataset = torch.utils.data.TensorDataset(batch_obs, batch_actions, batch_log_probs, batch_rtgs, A_k)\n\u001b[32m     31\u001b[39m dataloader = torch.utils.data.DataLoader(dataset,batch_size=config[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m], shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mupdate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepsilon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mupdate_models\u001b[39m\u001b[34m(dataloader, actor, critic, optimizer_actor, optimizer_critic, epsilon, n_epochs)\u001b[39m\n\u001b[32m     12\u001b[39m b_ak = batch[\u001b[32m4\u001b[39m].to(device)\n\u001b[32m     13\u001b[39m V = critic(b_obs).squeeze()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m curr_log_probs = \u001b[43mactor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m ratios = torch.exp(curr_log_probs - b_log_probs)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# On calcule la première partie de la perte \u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mPPONetwork.evaluate\u001b[39m\u001b[34m(self, input, actions)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, actions):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     dist = \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcov_mat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     log_probs = dist.log_prob(actions)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/club d'IA/tutoriels-cia/Tuto3_RL/tutoCiaEnv/lib/python3.12/site-packages/torch/distributions/multivariate_normal.py:189\u001b[39m, in \u001b[36mMultivariateNormal.__init__\u001b[39m\u001b[34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = scale_tril\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m covariance_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# precision_matrix is not None\u001b[39;00m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = _precision_to_scale_tril(precision_matrix)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(42, workers=True)\n",
    "\n",
    "num_eval_episodes = 10\n",
    "env_name = \"LunarLanderContinuous-v3\"\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 50 == 0    # Record every 50 episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} iterations...\")\n",
    "\n",
    "config = {\n",
    "    \"num_eval_episodes\":num_eval_episodes,\n",
    "    'n_actors':20,\n",
    "    'n_epochs':5,\n",
    "    \"batch_size\" : 64,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"lr\" : 3e-4,\n",
    "    \"epsilon\" : 0.2,\n",
    "    \"hidden_layer_size\":64,\n",
    "}\n",
    "\n",
    "train_model(env, config)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.mean(env.return_queue)\n",
    "avg_length = np.mean(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > 200) / len(env.return_queue):.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
