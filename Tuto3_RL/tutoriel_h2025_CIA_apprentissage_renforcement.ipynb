{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9be702-842c-4385-b00f-e87984963db6",
   "metadata": {},
   "source": [
    "# Apprentissage par renforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2109f046-5a00-48c9-abb2-28bd5546267c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T22:47:19.925691Z",
     "iopub.status.busy": "2025-09-24T22:47:19.925239Z",
     "iopub.status.idle": "2025-09-24T22:47:26.978078Z",
     "shell.execute_reply": "2025-09-24T22:47:26.977828Z",
     "shell.execute_reply.started": "2025-09-24T22:47:19.925664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !uv pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfaf220-f9e6-47e8-be00-5b88fd7f5d29",
   "metadata": {},
   "source": [
    "# Essayez les jeux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158f52-c34b-4c84-805f-32c48158051d",
   "metadata": {},
   "source": [
    "Vous pouvez essayer un jeu, vous avez le choix entre \"Car\", \"Mountain\" et \"Lunar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ebad210-4d01-4432-b316-44518ba0e690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:03:24.345605Z",
     "iopub.status.busy": "2025-09-24T23:03:24.343403Z",
     "iopub.status.idle": "2025-09-24T23:03:40.635991Z",
     "shell.execute_reply": "2025-09-24T23:03:40.631750Z",
     "shell.execute_reply.started": "2025-09-24T23:03:24.345504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mathieubazinet/Desktop/club d'IA/tutoriels-cia/Tuto3_RL/tutoCiaEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "!python play.py --game=\"Lunar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26807f52-1f19-409f-828c-4221a8aa6a73",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2aa06-fd5b-46a3-b2a0-d7248a3acd13",
   "metadata": {},
   "source": [
    "<img src=\"./images/deep-q-learning.jpg\" width=\"70%\" class=\"center\"/>\n",
    "\n",
    "[source](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fdac69e-269c-4a03-a092-ccf07e91035d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:43:04.125455Z",
     "iopub.status.busy": "2025-09-24T23:43:04.124050Z",
     "iopub.status.idle": "2025-09-24T23:43:04.140524Z",
     "shell.execute_reply": "2025-09-24T23:43:04.138725Z",
     "shell.execute_reply.started": "2025-09-24T23:43:04.125385Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3186880370.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31melse\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def update_model(model, frozen_model, batch_size, memory, gamma):\n",
    "    with torch.no_grad():\n",
    "        batch = Transition(*zip(*memory.sample(batch_size)))\n",
    "        all_states = torch.tensor(batch.state, device=model.device)\n",
    "        all_next_states = batch.next_state\n",
    "        rewards = torch.tensor(batch.rewards, device=model.device)\n",
    "        \n",
    "        mask = torch.tensor([next_state is not None for next_state in all_next_states], dtype=torch.int32)\n",
    "        \n",
    "        targets = rewards + gamma * mask * frozen_model(all_states).max(1)\n",
    "        ys = model(all_states).gather(1, batch.action)\n",
    "        loss = (targets - ys).pow(2).mean()\n",
    "\n",
    "        # optimize and clip the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcd024cb-ee77-4709-af43-3b90f03a16e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:37:30.488724Z",
     "iopub.status.busy": "2025-09-24T23:37:30.488281Z",
     "iopub.status.idle": "2025-09-24T23:37:30.496112Z",
     "shell.execute_reply": "2025-09-24T23:37:30.495188Z",
     "shell.execute_reply.started": "2025-09-24T23:37:30.488698Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, max_capacity):\n",
    "        self.memory = []\n",
    "        self.max_capacity = max_capacity\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        transition = Transition(state, action, next_state, reward)\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.max_capacity:\n",
    "            val = self.memory.pop(0)\n",
    "            assert len(self.memory) == self.max_capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b559f-408d-4b24-9a6e-8efbf4be68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_obs = n_obs\n",
    "        self.n_actions = n_actions\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_obs, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, self.n_actions)\n",
    "        )\n",
    "\n",
    "    def foward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4a3f066-6e20-4e41-a3f1-2a024f96edbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T17:02:45.031441Z",
     "iopub.status.busy": "2025-09-21T17:02:45.030958Z",
     "iopub.status.idle": "2025-09-21T17:02:45.042920Z",
     "shell.execute_reply": "2025-09-21T17:02:45.037167Z",
     "shell.execute_reply.started": "2025-09-21T17:02:45.031408Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(env, num_eval_episodes):\n",
    "    epsilon = 0.01\n",
    "    obs, info = env.reset()\n",
    "    size_observation_space = len(obs)\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    n_step_update = 50\n",
    "    max_capacity = 1500\n",
    "    batch_size = 64\n",
    "    \n",
    "    # Initialize replay memory D to capacity N\n",
    "    memory = ReplayMemory(max_capacity=max_capacity)\n",
    "    # Initialize action-value function Q with random weights theta\n",
    "    Q_model = DQN(size_observation_space, n_actions)\n",
    "    \n",
    "    # Initialize target action-value function \\hat{Q} with weights theta^-1 = theta\n",
    "    Q_hat_model = deepcopy(Q_model)\n",
    "    \n",
    "    for episode_num in range(num_eval_episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        epsilon_greedy = torch.distributions.bernoulli.Bernoulli(torch.tensor([epsilon]))\n",
    "    \n",
    "        for n_step in range(env._max_episode_steps):\n",
    "            # Add epsilon decay\n",
    "            if bool(m.sample().item()):\n",
    "                action = env.action_space.sample()  # Random policy for demonstration\n",
    "            else:\n",
    "                action = np.argmax(Q_model(obs))\n",
    "    \n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_terminated = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if episode_terminated:\n",
    "                memory.push(obs, action, None, reward)\n",
    "            else:\n",
    "                memory.push(obs, action, new_obs, reward)\n",
    "            obs = new_obs\n",
    "\n",
    "            if len(memory) >= batch_size:\n",
    "                ...\n",
    "\n",
    "            if n_step % n_step_update == 0:\n",
    "                Q_hat_model = deepcopy(Q_model)\n",
    "                \n",
    "            if episode_terminated:\n",
    "                break\n",
    "    \n",
    "        print(f\"Episode {episode_num + 1}: {step_count} steps, reward = {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf2fac59-4237-4ba1-8760-dd763ba709c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T17:02:50.654669Z",
     "iopub.status.busy": "2025-09-21T17:02:50.654223Z",
     "iopub.status.idle": "2025-09-21T17:02:51.254622Z",
     "shell.execute_reply": "2025-09-21T17:02:51.254317Z",
     "shell.execute_reply.started": "2025-09-21T17:02:50.654640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathieubazinet/Desktop/club d'IA/tutoriels-cia/Tuto3_RL/tutoCiaEnv/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/mathieubazinet/Desktop/club d'IA/tutoriels-cia/Tuto3_RL/MountainCar-v0_agent folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 4 episodes...\n",
      "Episode 1: 200 steps, reward = -200.0\n",
      "Episode 2: 200 steps, reward = -200.0\n",
      "Episode 3: 200 steps, reward = -200.0\n",
      "Episode 4: 200 steps, reward = -200.0\n",
      "\n",
      "Evaluation Summary:\n",
      "Episode durations: [0.174245, 0.001743, 0.001544, 0.001404]\n",
      "Episode rewards: [-200.0, -200.0, -200.0, -200.0]\n",
      "Episode lengths: [200, 200, 200, 200]\n",
      "\n",
      "Average reward: -800.00 ± 0.00\n",
      "Average episode length: 800.0 steps\n",
      "Success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "num_eval_episodes = 4\n",
    "env_name = \"MountainCar-v0\"\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 50 == 0    # Record every episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} episodes...\")\n",
    "\n",
    "\n",
    "train_model(env, num_eval_episodes)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'\\nEvaluation Summary:')\n",
    "print(f'Episode durations: {list(env.time_queue)}')\n",
    "print(f'Episode rewards: {list(env.return_queue)}')\n",
    "print(f'Episode lengths: {list(env.length_queue)}')\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.sum(env.return_queue)\n",
    "avg_length = np.sum(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > 0) / len(env.return_queue):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1f41b-0b4f-4f1e-992e-07b95ba271d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:30:19.511896Z",
     "iopub.status.busy": "2025-09-21T15:30:19.511255Z",
     "iopub.status.idle": "2025-09-21T15:31:06.262141Z",
     "shell.execute_reply": "2025-09-21T15:31:06.261854Z",
     "shell.execute_reply.started": "2025-09-21T15:30:19.511849Z"
    }
   },
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48a621bd-a457-42a9-bd11-7e1580cbb233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T17:06:31.317754Z",
     "iopub.status.busy": "2025-09-21T17:06:31.317152Z",
     "iopub.status.idle": "2025-09-21T17:06:31.325919Z",
     "shell.execute_reply": "2025-09-21T17:06:31.322215Z",
     "shell.execute_reply.started": "2025-09-21T17:06:31.317717Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a732e99-8c45-4b1f-8075-08a042b7bcc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T17:17:04.911851Z",
     "iopub.status.busy": "2025-09-21T17:17:04.911329Z",
     "iopub.status.idle": "2025-09-21T17:17:04.919311Z",
     "shell.execute_reply": "2025-09-21T17:17:04.918554Z",
     "shell.execute_reply.started": "2025-09-21T17:17:04.911812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f65fd1-dd8d-464c-97d9-a892cf682097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:38:47.354474Z",
     "iopub.status.busy": "2025-09-21T15:38:47.353950Z",
     "iopub.status.idle": "2025-09-21T15:38:47.554827Z",
     "shell.execute_reply": "2025-09-21T15:38:47.554595Z",
     "shell.execute_reply.started": "2025-09-21T15:38:47.354439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 4 episodes...\n",
      "Episode 1: 58 steps, reward = -113.3143971493336\n",
      "Episode 2: 96 steps, reward = -302.28691844428306\n",
      "Episode 3: 115 steps, reward = -106.40357398727691\n",
      "Episode 4: 70 steps, reward = -86.86919655459616\n",
      "\n",
      "Evaluation Summary:\n",
      "Episode durations: [0.081562, 0.005922, 0.007073, 0.003183]\n",
      "Episode rewards: [-113.3143971493336, -302.28691844428306, -106.40357398727691, -86.86919655459616]\n",
      "Episode lengths: [58, 96, 115, 70]\n",
      "\n",
      "Average reward: -608.87 ± 87.18\n",
      "Average episode length: 339.0 steps\n",
      "Success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "num_eval_episodes = 4\n",
    "env_name = \"LunarLander-v3\"  # Replace with your environmentLunar Lander\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 50 == 0    # Record every episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} episodes...\")\n",
    "\n",
    "for episode_num in range(num_eval_episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        # Replace this with your trained agent's policy\n",
    "        action = env.action_space.sample()  # Random policy for demonstration\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    print(f\"Episode {episode_num + 1}: {step_count} steps, reward = {episode_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'\\nEvaluation Summary:')\n",
    "print(f'Episode durations: {list(env.time_queue)}')\n",
    "print(f'Episode rewards: {list(env.return_queue)}')\n",
    "print(f'Episode lengths: {list(env.length_queue)}')\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.sum(env.return_queue)\n",
    "avg_length = np.sum(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > 0) / len(env.return_queue):.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
