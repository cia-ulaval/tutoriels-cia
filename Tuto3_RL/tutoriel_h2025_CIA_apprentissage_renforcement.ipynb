{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9be702-842c-4385-b00f-e87984963db6",
   "metadata": {},
   "source": [
    "# Apprentissage par renforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32f857",
   "metadata": {},
   "source": [
    "Le but de ce tutoriel est de vous montrer une implémentation simple de deux algorithmes très bien connus dans le domaine : le *Deep Q-Learning* et le *Proximal Policy Optimization*. Il y a plusieurs `TODO` à des endroits où vous devrez ajouter du code. La solution se trouve dans le fichier `solution_tutoriel_apprentissage_renforcement.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109f046-5a00-48c9-abb2-28bd5546267c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import lightning as L\n",
    "from torch.distributions import MultivariateNormal\n",
    "from tqdm import tqdm, trange\n",
    "from utils import get_device, verify_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfaf220-f9e6-47e8-be00-5b88fd7f5d29",
   "metadata": {},
   "source": [
    "# Essayez les jeux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158f52-c34b-4c84-805f-32c48158051d",
   "metadata": {},
   "source": [
    "Vous pouvez essayer un jeu, vous avez le choix entre \"Car\", \"Mountain\" et \"Lunar\". Vous pouvez jouer avec les touches \"a\", \"s\", \"d\" et \"w\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebad210-4d01-4432-b316-44518ba0e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python play.py --game=\"Mountain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26807f52-1f19-409f-828c-4221a8aa6a73",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bd134-ccb4-4645-83a9-51f926ef44f6",
   "metadata": {},
   "source": [
    "Cette section du laboratoire est fortement inspirée de ces deux tutoriels suivants : \n",
    "1. https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm\n",
    "\n",
    "2. https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2aa06-fd5b-46a3-b2a0-d7248a3acd13",
   "metadata": {},
   "source": [
    "<img src=\"./images/deep-q-learning.jpg\" width=\"70%\" class=\"center\"/>\n",
    "\n",
    "[source](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1808ca-66a6-48c5-a47a-fa23ebf64aa1",
   "metadata": {},
   "source": [
    "Commençons par implémenter le *replay buffer*. Le buffer a une taille maximale, donc lorsque le buffer est plein, on efface les données les plus anciennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd024cb-ee77-4709-af43-3b90f03a16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_capacity):\n",
    "        self.memory = [] # Choisissez la structure de données que vous préférez\n",
    "        self.max_capacity = max_capacity\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        transition = Transition(state, action, next_state, reward)\n",
    "        # TODO: Ajoutez les données à la mémoire, en s'assurant que le buffer n'est pas plein.\n",
    "        ...\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # TODO : piger aléatoirement une batch de taille batch_size\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135f4ec-abdc-463b-b50a-7b76d67bebed",
   "metadata": {},
   "source": [
    "On va maintenant implémenter le réseau de neurones. Ils suggèrent un réseau de neurones avec deux couches cachées de 128 neurones, donc c'est ce qu'on utilise ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b559f-408d-4b24-9a6e-8efbf4be68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_obs, n_actions, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.n_obs = n_obs\n",
    "        self.n_actions = n_actions\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_obs, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, self.n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c1fa0-c070-47f1-9be9-a4f1476bc133",
   "metadata": {},
   "source": [
    "On implémente l'étape d'entraînement de l'algorithme présenté plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdac69e-269c-4a03-a092-ccf07e91035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, frozen_model, batch_size, memory, gamma, optimizer):\n",
    "    model.train()\n",
    "    # Allez chercher une batch dans la mémoire\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    all_states = torch.tensor(np.stack(batch.state), device=get_device())\n",
    "    all_next_states = batch.next_state\n",
    "\n",
    "    # TODO : Créez un tenseur qui vérifie que next_state est un état valide et non un état terminal. Il doit contenir\n",
    "    # True si next_state est valide.\n",
    "    mask = torch.tensor(...\n",
    "                        , dtype=torch.bool, device=get_device())\n",
    "\n",
    "    # TODO : Créez un tenseur qui contient tous les next_state valides .\n",
    "    non_final_next_states = torch.tensor(np.stack(...\n",
    "                                                 ), device=get_device())\n",
    "\n",
    "    rewards = torch.tensor(batch.reward, device=get_device())\n",
    "    actions = torch.tensor(batch.action, device=get_device()).unsqueeze(1)\n",
    "    targets = rewards\n",
    "    with torch.no_grad():\n",
    "        targets[mask] += gamma * frozen_model(non_final_next_states).max(1).values\n",
    "    ys = model(all_states).gather(1, actions).reshape(-1,)\n",
    "\n",
    "    # TODO : Calculez la moyenne de la distance au carré entre targets et ys.\n",
    "    loss = ...\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # On clip le gradient pour assurer la stabilité\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2765229-d272-48c6-bb5a-fd9f5e2f3424",
   "metadata": {},
   "source": [
    "On implémente maintenant la boucle d'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3f066-6e20-4e41-a3f1-2a024f96edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, config):\n",
    "    obs, info = env.reset()\n",
    "    size_observation_space = len(obs)\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    epsilon = config['epsilon_start']\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    memory = ReplayMemory(max_capacity=config['max_capacity'])\n",
    "    # Initialize action-value function Q with random weights theta\n",
    "    Q_model = DQN(size_observation_space, n_actions, config['hidden_layer_size'])\n",
    "\n",
    "    # Initialize target action-value function \\hat{Q} with weights theta^-1 = theta\n",
    "    Q_hat_model = deepcopy(Q_model)\n",
    "\n",
    "    Q_model.eval()\n",
    "    Q_hat_model.eval()\n",
    "    Q_model.to(get_device())\n",
    "    Q_hat_model.to(get_device())\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_model.parameters(), lr=config['lr'])\n",
    "\n",
    "    global_step_count = 0\n",
    "    for episode_num in range(config['num_eval_episodes']):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        # On crée notre distribution pour le compromis exploration-exploitation\n",
    "        epsilon_greedy = torch.distributions.bernoulli.Bernoulli(torch.tensor([epsilon]))\n",
    "        for n_step in range(env.spec.max_episode_steps):\n",
    "            # Epsilon decay\n",
    "            if bool(epsilon_greedy.sample().item()):\n",
    "                action = env.action_space.sample()  # Pige aléatoire\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(Q_model(torch.tensor(obs, device=get_device()))).item()\n",
    "\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_terminated = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            global_step_count += 1\n",
    "\n",
    "            # On ajoute les données dans la mémoire. Si l'état est terminal, on ajoute None au lieu de l'état\n",
    "            if episode_terminated:\n",
    "                memory.push(obs, action, None, reward)\n",
    "            else:\n",
    "                memory.push(obs, action, new_obs, reward)\n",
    "            obs = new_obs\n",
    "\n",
    "            # Dès qu'on a assez de données, on se met à entraîner le modèle\n",
    "            if len(memory) >= config['batch_size']:\n",
    "                update_model(Q_model, Q_hat_model, config['batch_size'], memory, config['gamma'], optimizer)\n",
    "\n",
    "            # Après plusieurs étapes, on met à jour notre modèle gelé\n",
    "            if global_step_count % config['n_step_update'] == 0:\n",
    "                Q_hat_model = deepcopy(Q_model)\n",
    "\n",
    "            if episode_terminated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode_num + 1}: {step_count} steps, reward = {episode_reward}\")\n",
    "        epsilon *= pow(config['epsilon_end']/config['epsilon_start'], 1/num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bcb2a-7699-4126-80e7-dd5685744662",
   "metadata": {},
   "source": [
    "Voici le code qui permet de rouler l'entièreté de l'algorithme. Vous pouvez jouer avec les hyperparamètres, mais ceux que j'ai utilisés fonctionnent bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fac59-4237-4ba1-8760-dd763ba709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.pytorch.seed_everything(42, workers=True)\n",
    "\n",
    "num_eval_episodes = 450\n",
    "env_name = \"MountainCar-v0\"\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 10 == 0    # Record every episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} episodes...\")\n",
    "\n",
    "config = {\n",
    "    \"num_eval_episodes\":num_eval_episodes,\n",
    "    \"n_step_update\": 10,\n",
    "    \"max_capacity\" : 10000,\n",
    "    \"batch_size\" : 128,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"lr\" : 3e-4,\n",
    "    \"epsilon_start\" : 0.9,\n",
    "    \"epsilon_end\" : 0.01,\n",
    "    \"hidden_layer_size\":128,\n",
    "}\n",
    "\n",
    "train_model(env, config)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.mean(env.return_queue)\n",
    "avg_length = np.mean(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > -200) / len(env.return_queue):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1f41b-0b4f-4f1e-992e-07b95ba271d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:30:19.511896Z",
     "iopub.status.busy": "2025-09-21T15:30:19.511255Z",
     "iopub.status.idle": "2025-09-21T15:31:06.262141Z",
     "shell.execute_reply": "2025-09-21T15:31:06.261854Z",
     "shell.execute_reply.started": "2025-09-21T15:30:19.511849Z"
    }
   },
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df2527-2e4c-4e1f-83ae-51bf76248ac4",
   "metadata": {},
   "source": [
    "Cette section du laboratoire est fortement inspirée de cette implémentation : \n",
    "1. https://github.com/ericyangyu/PPO-for-Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563fbcd-2789-481a-a732-7ff187e4f6ce",
   "metadata": {},
   "source": [
    "<img src=\"./images/ppo_algo.png\" width=\"70%\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474db46-1ecc-4111-9392-4301afc56b6d",
   "metadata": {},
   "source": [
    "Les deux réseaux de neurones ont la même architecture, donc j'ai créé seulement une classe pour faire les deux réseaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3a3f1-8a7b-4aae-9121-4cb0227de90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPONetwork(torch.nn.Module):\n",
    "    def __init__(self, n_obs, out_dim, hidden_size=64, std_val=0.5):\n",
    "        super().__init__()\n",
    "        self.n_obs = n_obs\n",
    "        self.out_dim = out_dim\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_obs, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, self.out_dim)\n",
    "        )\n",
    "        self.cov_mat = torch.diag(torch.full(size=(self.out_dim,), fill_value=std_val))\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "    def get_action(self, input):\n",
    "        moyenne = self(input)\n",
    "        dist = MultivariateNormal(moyenne, self.cov_mat.to(get_device()))\n",
    "\n",
    "        # TODO : On pige une action dans la distribution\n",
    "        action = ...\n",
    "\n",
    "        # TODO : On calcule la log_probabilité de l'action en fonction de la distribution\n",
    "        log_prob = ...\n",
    "\n",
    "        return action.cpu().numpy(), log_prob.detach().cpu().item()\n",
    "\n",
    "    def evaluate(self, input, actions):\n",
    "        dist = MultivariateNormal(self(input), self.cov_mat.to(get_device()))\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853deee-9755-4ed9-8506-e35fca2637dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:06:23.657643Z",
     "iopub.status.busy": "2025-10-06T14:06:23.655856Z",
     "iopub.status.idle": "2025-10-06T14:06:23.667876Z",
     "shell.execute_reply": "2025-10-06T14:06:23.666699Z",
     "shell.execute_reply.started": "2025-10-06T14:06:23.657559Z"
    }
   },
   "source": [
    "On va maintenant estimer les \"avantages\". Pour ce faire, on va calculer la somme actualisée avec le paramètre gamma pour chaque épisodes. \n",
    "<img src=\"./images/advantages.png\" width=\"50%\" class=\"center\"/>\n",
    "\n",
    "Plus exactement, pour chaque épisode, on va chercher à calculer :\n",
    "$\\sum_{t=0}^T \\gamma^t r_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95769853-7743-41f0-8d38-6d854c8289bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_estimated_advantages(batch_rewards, gamma):\n",
    "    batch_rtgs = []\n",
    "    # On calcule les rewards diminués pour chaque épisode\n",
    "    for ep_rewards in reversed(batch_rewards):\n",
    "\n",
    "        discounted_reward = 0  # Somme des rewards diminués\n",
    "\n",
    "        # On itère sur tous les rewards de l'épisode\n",
    "        for rew in reversed(ep_rewards):\n",
    "            # TODO : On additionne le reward actual avec les rewards obtenus précédemment multipliés par gamma.\n",
    "            discounted_reward = ...\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "    return batch_rtgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05dffea-1a0e-4738-82f6-10e4489f7f85",
   "metadata": {},
   "source": [
    "On fait jouer plein d'agents différents pour obtenir les données sur lesquelles on va s'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43309e-2f2c-405c-8363-67f0c82617c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, n_actors, iter_number, actor, critic):\n",
    "    batch_obs = []\n",
    "    batch_actions = []\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "\n",
    "    for n_actor in trange(n_actors, desc=\"Actors\"):\n",
    "        obs, info = env.reset(seed=(iter_number+1)*(n_actor+1))\n",
    "        ep_rewards = []\n",
    "        for n_step in range(env.spec.max_episode_steps):\n",
    "            action, log_prob = actor.get_action(torch.tensor(obs, device=get_device()))\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            ep_rewards.append(reward)\n",
    "            batch_obs.append(obs)\n",
    "            batch_actions.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "            episode_terminated = terminated or truncated\n",
    "\n",
    "            if episode_terminated:\n",
    "                break\n",
    "        batch_rewards.append(ep_rewards)\n",
    "\n",
    "    batch_obs = torch.tensor(np.array(batch_obs), dtype=torch.float)\n",
    "    batch_actions = torch.tensor(np.array(batch_actions), dtype=torch.float)\n",
    "    batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float)\n",
    "    \n",
    "    return batch_obs, batch_actions, batch_log_probs, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ca608-d70d-4cd2-8cda-65d61e3137fd",
   "metadata": {},
   "source": [
    "On met maintenant à jour les réseaux de neurones. On calcule la perte avec l'équation suivante. Le ratio $r_t$ est calculé pour vous.\n",
    "\n",
    "<img src=\"./images/l_clip.png\" width=\"50%\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741285b-f50a-4f01-b6f3-4dc17bd9cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_models(dataloader, actor, critic, optimizer_actor, optimizer_critic, epsilon, n_epochs):\n",
    "        actor.train()\n",
    "        critic.train()\n",
    "        device = get_device()\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for _ in trange(n_epochs, desc=\"Epochs\"):\n",
    "            for batch in dataloader:\n",
    "                b_obs = batch[0].to(device)\n",
    "                b_acts = batch[1].to(device)\n",
    "                b_log_probs = batch[2].to(device)\n",
    "                b_rtgs = batch[3].to(device)\n",
    "                b_ak = batch[4].to(device)\n",
    "                V = critic(b_obs).squeeze()\n",
    "                curr_log_probs = actor.evaluate(b_obs, b_acts)\n",
    "                ratios = torch.exp(curr_log_probs - b_log_probs)\n",
    "\n",
    "                # TODO : On calcule la première partie de la perte \n",
    "                surr1 = ...\n",
    "                # TODO : On calcule la deuxième partie de la perte, avec la fonction torch.clamp\n",
    "                surr2 = ...\n",
    "\n",
    "                # On calcule la perte pour l'acteur\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                optimizer_actor.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                optimizer_actor.step()\n",
    "\n",
    "                # On calcule la perte pour le critique\n",
    "                V, b_rtgs = verify_shape(V, b_rtgs)\n",
    "                critic_loss = loss_fn(V, b_rtgs)\n",
    "\n",
    "                optimizer_critic.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                optimizer_critic.step()\n",
    "        actor.eval()\n",
    "        critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2085-3a0c-4949-a3e1-69d62deb2856",
   "metadata": {},
   "source": [
    "Voici la boucle d'entraînement. On fait notre \"rollout\", où on crée les données sur lesquelles on va s'entraîner. Ensuite, on estimer l'avantage et on entraîne le modèle, avec les méthodes qu'on a implémentés plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b5bfb-dfa7-4435-b741-3817bef54f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, config):\n",
    "    obs, info = env.reset()\n",
    "    size_observation_space = len(obs)\n",
    "    n_actions = env.action_space.shape[0]\n",
    "\n",
    "    actor = PPONetwork(size_observation_space, n_actions, config['hidden_layer_size'])\n",
    "    critic = PPONetwork(size_observation_space, 1, config['hidden_layer_size'])\n",
    "\n",
    "    optimizer_actor = torch.optim.Adam(actor.parameters(), lr=config['lr'], eps=1e-5)\n",
    "    optimizer_critic = torch.optim.Adam(critic.parameters(), lr=config['lr'], eps=1e-5)\n",
    "\n",
    "    actor.to(get_device())\n",
    "    actor.eval()\n",
    "    critic.to(get_device())\n",
    "    critic.eval()\n",
    "\n",
    "    for i in range(config['num_eval_episodes']):\n",
    "        batch_obs, batch_actions, batch_log_probs, batch_rewards = rollout(env, config['n_actors'], i, actor, critic)\n",
    "\n",
    "        final_rewards = np.array([sum(ep_rewards) for ep_rewards in batch_rewards])\n",
    "        print(f\"Average reward : {final_rewards.mean():.2f}   Max final reward : {final_rewards.max():.2f}\")\n",
    "        print(f\"Pourcentage de réussite après {i} itérations: {100*np.count_nonzero(final_rewards>=200)/final_rewards.shape[0]:.2f}%\")\n",
    "\n",
    "        batch_rtgs = calculate_estimated_advantages(batch_rewards, config['gamma'])\n",
    "        \n",
    "        V = critic(batch_obs.to(get_device())).squeeze()\n",
    "        A_k = batch_rtgs.to(get_device()) - V.detach()\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # Normalization trick\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(batch_obs, batch_actions, batch_log_probs, batch_rtgs, A_k)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,batch_size=config['batch_size'], shuffle=True)\n",
    "        update_models(dataloader, actor, critic, optimizer_actor, optimizer_critic, config['epsilon'], config['n_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb3b33-1e69-4cc2-b038-41e30b0d23ad",
   "metadata": {},
   "source": [
    "Voici le code qui permet de rouler l'entièreté de l'algorithme. Vous pouvez jouer avec les hyperparamètres, mais ceux que j'ai utilisés fonctionnent bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f65fd1-dd8d-464c-97d9-a892cf682097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L.pytorch.seed_everything(42, workers=True)\n",
    "\n",
    "num_eval_episodes = 10\n",
    "env_name = \"LunarLanderContinuous-v3\"\n",
    "\n",
    "# Create environment with recording capabilities\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")  # rgb_array needed for video recording\n",
    "\n",
    "# Add video recording for every episode\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=env_name + \"_agent\",    # Folder to save videos\n",
    "    name_prefix=\"eval\",               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: x % 50 == 0    # Record every 50 episode\n",
    ")\n",
    "\n",
    "# Add episode statistics tracking\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "print(f\"Starting evaluation for {num_eval_episodes} iterations...\")\n",
    "\n",
    "config = {\n",
    "    \"num_eval_episodes\":num_eval_episodes,\n",
    "    'n_actors':20,\n",
    "    'n_epochs':5,\n",
    "    \"batch_size\" : 64,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"lr\" : 3e-4,\n",
    "    \"epsilon\" : 0.2,\n",
    "    \"hidden_layer_size\":64,\n",
    "}\n",
    "\n",
    "train_model(env, config)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calculate some useful metrics\n",
    "avg_reward = np.mean(env.return_queue)\n",
    "avg_length = np.mean(env.length_queue)\n",
    "std_reward = np.std(env.return_queue)\n",
    "\n",
    "print(f'\\nAverage reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "print(f'Average episode length: {avg_length:.1f} steps')\n",
    "print(f'Success rate: {sum(1 for r in env.return_queue if r > 200) / len(env.return_queue):.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
